{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc287b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf1b779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\assi01\\\\Desktop\\\\projects\\\\AirTravel_Sentiment_Analysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd03b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b241ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\assi01\\\\Desktop\\\\projects\\\\AirTravel_Sentiment_Analysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546f7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class TextProcessingConfig:\n",
    "    root_dir: Path\n",
    "    train_tokenized_data_path: Path\n",
    "    test_tokenized_data_path: Path\n",
    "    val_tokenized_data_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    val_data_path: Path\n",
    "    params_model_name: str\n",
    "    params_text_col: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24eb370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airTravelSentimentAnalysis.constants import *\n",
    "from airTravelSentimentAnalysis.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b55666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_text_processing_config(self) -> TextProcessingConfig:\n",
    "        config = self.config.text_processing\n",
    "        config[\"train_data_path\"] = self.config.data_preprocessing.train_data_path\n",
    "        config[\"test_data_path\"] = self.config.data_preprocessing.test_data_path\n",
    "        config[\"val_data_path\"] = self.config.data_preprocessing.val_data_path\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        text_processing_config = TextProcessingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            train_data_path=Path(config.train_data_path),\n",
    "            test_data_path=Path(config.test_data_path),\n",
    "            val_data_path=Path(config.val_data_path),\n",
    "            train_tokenized_data_path=Path(config.train_tokenized_data_path),\n",
    "            test_tokenized_data_path=Path(config.test_tokenized_data_path),\n",
    "            val_tokenized_data_path=Path(config.val_tokenized_data_path),\n",
    "            params_model_name=self.params.MODEL_NAME,\n",
    "            params_text_col= self.params.TEXT_COL,\n",
    "        )\n",
    "\n",
    "        return text_processing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8fbcc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TextProcessing:\n",
    "    def __init__(self, config: TextProcessingConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def loadData(self):\n",
    "        self.train_df = pd.read_csv( self.config.train_data_path, encoding='utf-8')\n",
    "        self.test_df = pd.read_csv( self.config.test_data_path, encoding='utf-8')\n",
    "        self.val_df = pd.read_csv( self.config.val_data_path, encoding='utf-8')\n",
    "        return self.train_df, self.test_df, self.val_df\n",
    "    \n",
    "    def createHuggingFaceDataset(self):\n",
    "        self.train_dataset = Dataset.from_pandas(self.train_df)\n",
    "        self.test_dataset = Dataset.from_pandas(self.test_df)\n",
    "        self.val_dataset = Dataset.from_pandas(self.val_df)\n",
    "        self.ds = DatasetDict()\n",
    "        self.ds[\"train\"] = self.train_dataset\n",
    "        self.ds[\"test\"] = self.test_dataset\n",
    "        self.ds[\"val\"] = self.val_dataset\n",
    "        return self.ds\n",
    "    \n",
    "    def tokenize_fn(self,batch):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.params_model_name)\n",
    "        return tokenizer(batch[self.config.params_text_col], truncation=True,padding=True)\n",
    "    \n",
    "    def tokenizeData(self):\n",
    "        self.tokenized_datasets = self.ds.map(self.tokenize_fn, batched=True)\n",
    "        self.tokenized_datasets.save_to_disk(self.config.root_dir)\n",
    "        return self.tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6ab47c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-22 22:34:17,374: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-22 22:34:17,376: INFO: common: yaml file content: {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://drive.google.com/file/d/1taIeW6BZHqucmJbccEo92qfzZt5X_RTQ/view?usp=sharing', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'prepare_base_model': {'root_dir': 'artifacts/prepare_base_model', 'base_model_path': 'artifacts/prepare_base_model/base_model.h5', 'updated_base_model_path': 'artifacts/prepare_base_model/base_model_updated.h5', 'base_tokenizer_path': 'artifacts/prepare_base_tokenizer/base_model.h5', 'updated_base_tokenizer_path': 'artifacts/prepare_base_tokenizer/base_model_updated.h5'}, 'data_preprocessing': {'root_dir': 'artifacts/data_preprocessing', 'raw_data_file': 'artifacts/data_ingestion/bitext-travel-llm-chatbot-training-dataset.csv', 'train_data_path': 'artifacts/data_preprocessing/train.csv', 'test_data_path': 'artifacts/data_preprocessing/test.csv', 'val_data_path': 'artifacts/data_preprocessing/val.csv'}, 'text_processing': {'root_dir': 'artifacts/text_processing', 'train_tokenized_data_path': 'artifacts/text_processing/train_tokenized.csv', 'test_tokenized_data_path': 'artifacts/text_processing/test_tokenized.csv', 'val_tokenized_data_path': 'artifacts/text_processing/val_tokenized.csv'}} ]\n",
      "[2025-05-22 22:34:17,377: INFO: common: datapreprocessing file content: {'root_dir': 'artifacts/data_preprocessing', 'raw_data_file': 'artifacts/data_ingestion/bitext-travel-llm-chatbot-training-dataset.csv', 'train_data_path': 'artifacts/data_preprocessing/train.csv', 'test_data_path': 'artifacts/data_preprocessing/test.csv', 'val_data_path': 'artifacts/data_preprocessing/val.csv'} ]\n",
      "[2025-05-22 22:34:17,380: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-22 22:34:17,381: INFO: common: yaml file content: {'CHECKPOINT': 'distilbert-base-uncased', 'NUM_LABELS': 33, 'MAX_LENGTH': 512, 'BATCH_SIZE': 16, 'EPOCHS': 3, 'MODEL_DIR': 'artifacts/model', 'MODEL_NAME': 'distilbert-base-uncased', 'TEST_SIZE': 0.2, 'TRAIN_SIZE': 0.8, 'SEED': 42, 'RANDOM_STATE': 42, 'LABEL_COL': 'intent', 'TEXT_COL': 'instruction'} ]\n",
      "[2025-05-22 22:34:17,383: INFO: common: datapreprocessing file content: None ]\n",
      "[2025-05-22 22:34:17,386: INFO: common: created directory at: artifacts]\n",
      "[2025-05-22 22:34:17,387: INFO: common: created directory at: artifacts/text_processing]\n",
      "Load after processed data successfully\n",
      "                                         instruction  intent\n",
      "0                     can uhelp me to selext my seat      22\n",
      "1  I do not want my flight, wil you help me cance...       2\n",
      "2  i got to see my fucking flight insurance cover...      11\n",
      "3  I need fucking flight offers from Chicago to M...      12\n",
      "4  I'd like to modify my flight booking from Chic...       4\n",
      "[2025-05-22 22:34:17,430: INFO: 1541279884: Load after processed data successfully \n",
      "                                          instruction  intent\n",
      "0                     can uhelp me to selext my seat      22\n",
      "1  I do not want my flight, wil you help me cance...       2\n",
      "2  i got to see my fucking flight insurance cover...      11\n",
      "3  I need fucking flight offers from Chicago to M...      12\n",
      "4  I'd like to modify my flight booking from Chic...       4]\n",
      "Hugging face dataset created successfully\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 20260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 5066\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 6332\n",
      "    })\n",
      "})\n",
      "[2025-05-22 22:34:17,452: INFO: 1541279884: Hugging face dataset created successfully \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 20260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 5066\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 6332\n",
      "    })\n",
      "})]\n",
      "Hugging face dataset created successfully\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 20260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 5066\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 6332\n",
      "    })\n",
      "})\n",
      "[2025-05-22 22:34:17,473: INFO: 1541279884: Hugging face dataset created successfully \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 20260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 5066\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['instruction', 'intent'],\n",
      "        num_rows: 6332\n",
      "    })\n",
      "})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20260/20260 [00:20<00:00, 997.53 examples/s] \n",
      "Map: 100%|██████████| 5066/5066 [00:06<00:00, 752.76 examples/s] \n",
      "Map: 100%|██████████| 6332/6332 [00:06<00:00, 958.23 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 20260/20260 [00:00<00:00, 611923.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5066/5066 [00:00<00:00, 209057.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6332/6332 [00:00<00:00, 248850.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset created successfully\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'intent', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 20260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'intent', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5066\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['instruction', 'intent', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 6332\n",
      "    })\n",
      "})\n",
      "[2025-05-22 22:34:51,500: INFO: 1541279884: Tokenized dataset created successfully \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'intent', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 20260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'intent', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 5066\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['instruction', 'intent', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 6332\n",
      "    })\n",
      "})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from airTravelSentimentAnalysis import logger\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    text_processing_config = config.get_text_processing_config()\n",
    "    text_processing = TextProcessing(config=text_processing_config)\n",
    "    \n",
    "    train_df,test_df, val_df = text_processing.loadData()\n",
    "    print(\"Load after processed data successfully\")\n",
    "    print(train_df.head())\n",
    "    logger.info(\"Load after processed data successfully \\n %s\",train_df.head())\n",
    "    \n",
    "    ds = text_processing.createHuggingFaceDataset()\n",
    "    print(\"Hugging face dataset created successfully\")\n",
    "    print(ds)\n",
    "    logger.info(\"Hugging face dataset created successfully \\n %s\",ds)\n",
    "    \n",
    "    ds = text_processing.createHuggingFaceDataset()\n",
    "    print(\"Hugging face dataset created successfully\")\n",
    "    print(ds)\n",
    "    logger.info(\"Hugging face dataset created successfully \\n %s\",ds)\n",
    "    \n",
    "    ds = text_processing.tokenizeData()\n",
    "    print(\"Tokenized dataset created successfully\")\n",
    "    print(ds)\n",
    "    logger.info(\"Tokenized dataset created successfully \\n %s\",ds)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8095072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0396082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
