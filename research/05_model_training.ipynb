{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65805bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a751303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\assi01\\\\Desktop\\\\projects\\\\AirTravel_Sentiment_Analysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecfa4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504e3f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\assi01\\\\Desktop\\\\projects\\\\AirTravel_Sentiment_Analysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d841e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path\n",
    "    base_tokenizer_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    train_tokenized_data_path: Path\n",
    "    test_tokenized_data_path: Path\n",
    "    val_tokenized_data_path: Path\n",
    "    params_model_name: str\n",
    "    params_eval_strategy: str\n",
    "    params_save_strategy: str\n",
    "    params_learning_rate: float\n",
    "    params_per_device_train_batch_size: int\n",
    "    params_per_device_eval_batch_size: int\n",
    "    params_num_train_epochs: int\n",
    "    params_weight_decay: float\n",
    "    params_load_best_model_at_end: bool\n",
    "    params_metric_for_best_model: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a849d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airTravelSentimentAnalysis.constants import *\n",
    "from airTravelSentimentAnalysis.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422ca28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        config[\"base_model_path\"] = self.config.prepare_base_model.base_model_path\n",
    "        config[\"base_tokenizer_path\"] = self.config.prepare_base_model.base_tokenizer_path\n",
    "        params_training = self.params.TRAINING_ARGUMENTS\n",
    "        print(f\"Training arguments: {params_training}\")\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            base_model_path=Path(config.base_model_path),\n",
    "            base_tokenizer_path=Path(config.base_tokenizer_path),\n",
    "            model_path=Path(config.model_path),\n",
    "            tokenizer_path=Path(config.tokenizer_path),\n",
    "            train_tokenized_data_path=Path(config.train_tokenized_data_path),\n",
    "            test_tokenized_data_path=Path(config.test_tokenized_data_path),\n",
    "            val_tokenized_data_path=Path(config.val_tokenized_data_path),\n",
    "            params_model_name=self.params.MODEL_NAME,\n",
    "            params_eval_strategy=params_training.eval_strategy,\n",
    "            params_save_strategy=params_training.save_strategy,\n",
    "            params_learning_rate=params_training.learning_rate,\n",
    "            params_per_device_train_batch_size=params_training.per_device_train_batch_size,\n",
    "            params_per_device_eval_batch_size=params_training.per_device_eval_batch_size,\n",
    "            params_num_train_epochs=params_training.num_train_epochs,\n",
    "            params_weight_decay=params_training.weight_decay,\n",
    "            params_load_best_model_at_end=params_training.load_best_model_at_end,\n",
    "            params_metric_for_best_model=params_training.metric_for_best_model,\n",
    "        )\n",
    "\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eecb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q dagshub mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a655889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-24 11:29:57,201: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as ashish.student2025\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as ashish.student2025\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-24 11:29:57,208: INFO: helpers: Accessing as ashish.student2025]\n",
      "[2025-05-24 11:29:57,941: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/repos/ashish.student2025/AirTravel_SentimentAnalysis \"HTTP/1.1 200 OK\"]\n",
      "[2025-05-24 11:29:58,582: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ashish.student2025/AirTravel_SentimentAnalysis\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"ashish.student2025/AirTravel_SentimentAnalysis\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-24 11:29:58,587: INFO: helpers: Initialized MLflow to track repo \"ashish.student2025/AirTravel_SentimentAnalysis\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ashish.student2025/AirTravel_SentimentAnalysis initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository ashish.student2025/AirTravel_SentimentAnalysis initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-24 11:29:58,591: INFO: helpers: Repository ashish.student2025/AirTravel_SentimentAnalysis initialized!]\n"
     ]
    }
   ],
   "source": [
    "import dagshub\n",
    "dad = dagshub.init(\n",
    "    repo_owner=\"ashish.student2025\",\n",
    "    repo_name=\"AirTravel_SentimentAnalysis\",\n",
    "    mlflow=True,\n",
    ")\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\n",
    "    \"https://dagshub.com/ashish.student2025/AirTravel_SentimentAnalysis.mlflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cb8a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] =\"https://dagshub.com\\\\ashish.student2025\\AirTravel_SentimentAnalysis.mlflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18a1bace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ashish.student2025'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"MLFLOW_TRACKING_URI\").split(os.sep)[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975350cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade \"transformers>=4.37.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6dd85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a746ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from torchinfo import summary\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19880191",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m.remote\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = load_from_disk(\"artifacts/text_processing/train\")\n",
    "# tokenized_train_dataset = tokenized_train_dataset.rename_column(\"intent\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset=tokenized_train_dataset.select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "# import numpy as np\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#         print(\"*******************************************************************compute_metrics called +++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "#         logits, labels = eval_pred\n",
    "#         predictions = np.argmax(logits,axis=-1)  # Predicted class is the index of max logit\n",
    "#         precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "#         recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "#         eval_f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "#         accuracy = accuracy_score(labels, predictions)\n",
    "#         return {\n",
    "#             \"accuracy\": accuracy,\n",
    "#             \"precision\": precision,\n",
    "#             \"recall\": recall,\n",
    "#             \"f1\": f1,\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# from transformers import Trainer\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from datasets import load_from_disk\n",
    "# from torchinfo import summary\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"artifacts/model_training/model.h5\",\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,  # Learning rate was 2e-5\n",
    "#     per_device_train_batch_size=8,  # Batch size for training\n",
    "#     per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "#     num_train_epochs=4,  # Number of training epochs\n",
    "#     weight_decay=0.01,  # Weight decay for regularization\n",
    "#     load_best_model_at_end=True,  # Load best model after training\n",
    "#     metric_for_best_model=\"f1\",\n",
    "# )\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"artifacts/prepare_base_model/base_model.h5\"\n",
    "# )\n",
    "# print(\"*******************without peft*******************\")\n",
    "# print(summary(model))\n",
    "# print(\"*******************Model*******************\")\n",
    "# print(model)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"artifacts/model_training/tokenizer.h5\")\n",
    "# tokenized_train_dataset = load_from_disk(\"artifacts/text_processing/train\")\n",
    "# tokenized_train_dataset = tokenized_train_dataset.rename_column(\"intent\", \"labels\")\n",
    "# tokenized_test_dataset = load_from_disk(\"artifacts/text_processing/test\")\n",
    "# tokenized_val_dataset = load_from_disk(\"artifacts/text_processing/val\")\n",
    "# tokenized_val_dataset = tokenized_val_dataset.rename_column(\"intent\", \"labels\")\n",
    "# tokenized_train_dataset = tokenized_train_dataset.select(range(20))\n",
    "# tokenized_test_dataset = tokenized_test_dataset.select(range(20))\n",
    "# tokenized_val_dataset = tokenized_val_dataset.select(range(20))\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbccaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def compute_metrics(self,eval_pred):\n",
    "        print(\"*******************************************************************compute_metrics called +++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits,axis=-1)  # Predicted class is the index of max logit\n",
    "        precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "        recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "        f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "    def train(self):\n",
    "        print(self.config)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.model_path,\n",
    "            eval_strategy=self.config.params_eval_strategy,\n",
    "            save_strategy=self.config.params_save_strategy,\n",
    "            learning_rate=float(self.config.params_learning_rate),\n",
    "            per_device_train_batch_size=self.config.params_per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.params_per_device_eval_batch_size,\n",
    "            num_train_epochs=self.config.params_num_train_epochs,\n",
    "            weight_decay=self.config.params_weight_decay,\n",
    "            logging_strategy=\"epoch\",\n",
    "            load_best_model_at_end=self.config.params_load_best_model_at_end,\n",
    "            metric_for_best_model=self.config.params_metric_for_best_model,\n",
    "        )\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=1,\n",
    "            lora_alpha=1,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_lin\", \"v_lin\"]\n",
    "        )\n",
    "        # mlflow.log_params(lora_config.to_dict())\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.base_model_path)\n",
    "        print(\"*******************without peft*******************\")\n",
    "        print(summary(model))\n",
    "        print(\"*******************Model*******************\")\n",
    "        print(model)\n",
    "        print(\"*******************with peft*******************\")        \n",
    "        peft_model = get_peft_model(model, lora_config)\n",
    "        print(summary(peft_model))\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.base_tokenizer_path)\n",
    "        tokenized_train_dataset = load_from_disk(self.config.train_tokenized_data_path)\n",
    "        tokenized_train_dataset = tokenized_train_dataset.rename_column(\"intent\", \"labels\")\n",
    "        tokenized_test_dataset = load_from_disk(self.config.test_tokenized_data_path) \n",
    "        tokenized_test_dataset = tokenized_test_dataset.rename_column(\"intent\", \"labels\")\n",
    "        tokenized_val_dataset = load_from_disk(self.config.val_tokenized_data_path)\n",
    "        tokenized_val_dataset = tokenized_val_dataset.rename_column(\"intent\", \"labels\")\n",
    "        # tokenized_train_dataset = tokenized_train_dataset.select(range(20))\n",
    "        # tokenized_test_dataset = tokenized_test_dataset.select(range(20))\n",
    "        # tokenized_val_dataset = tokenized_val_dataset.select(range(20))\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train_dataset,\n",
    "            eval_dataset=tokenized_test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )\n",
    "        mlflow.set_experiment(\"Air Travel Sentiment Analysis\")\n",
    "        with mlflow.start_run() as run:\n",
    "            trainer.train()\n",
    "            metrics = trainer.evaluate()\n",
    "            mlflow.log_metrics(metrics)\n",
    "        trainer.save_model(self.config.model_path)\n",
    "        tokenizer.save_pretrained(self.config.tokenizer_path)\n",
    "        tuned_pipeline = pipeline(\n",
    "            task=\"text-classification\",\n",
    "            model=trainer.model,\n",
    "            batch_size=8,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "        model_config = {\"batch_size\": 8}\n",
    "        signature = mlflow.models.infer_signature(\n",
    "            [\"This is a test!\", \"And this is also a test.\"],\n",
    "            mlflow.transformers.generate_signature_output(\n",
    "                tuned_pipeline, [\"This is a test response!\", \"So is this.\"]\n",
    "            ),\n",
    "            params=model_config,\n",
    "        )\n",
    "        # Log the pipeline to the existing training run\n",
    "        with mlflow.start_run(run_id=run.info.run_id):\n",
    "            model_info = mlflow.transformers.log_model(\n",
    "                transformers_model=tuned_pipeline,\n",
    "                artifact_path=\"fine_tuned\",\n",
    "                signature=signature,\n",
    "                input_example=[\"Pass in a string\", \"And have it mark as spam or not.\"],\n",
    "                model_config=model_config,\n",
    "            )\n",
    "        print(\"Model saved in run %s\" % model_info.model_uri)\n",
    "        loaded = mlflow.transformers.load_model(model_uri=model_info.model_uri)\n",
    "\n",
    "        decoded_texts = ()\n",
    "        for i in range(len(tokenized_val_dataset)):\n",
    "            decoded_text = tokenizer.decode(\n",
    "                tokenized_val_dataset[i][\"input_ids\"],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "            b = list(decoded_texts)\n",
    "            b.append(decoded_text)\n",
    "            decoded_texts = tuple(b)\n",
    "        loaded(list(tokenized_val_dataset))\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43bc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"artifacts/model_training/model.h5\"\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"artifacts/model_training/tokenizer.h5\")\n",
    "# tuned_pipeline = pipeline(\n",
    "#     task=\"text-classification\",\n",
    "#     model=model,\n",
    "#     batch_size=8,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=\"cpu\",\n",
    "# )\n",
    "# loaded = mlflow.transformers.load_model(\n",
    "#     model_uri=\"runs:/294251af6d9d479598901ebdafc37b7e/fine_tuned\"\n",
    "# )\n",
    "# tokenized_val_dataset = load_from_disk(\"artifacts/text_processing/val\")\n",
    "# tokenized_val_dataset = tokenized_val_dataset.rename_column(\"intent\", \"labels\")\n",
    "# tokenized_val_dataset = tokenized_val_dataset.select(range(100))\n",
    "# decoded_texts = ()\n",
    "# for i in range(len(tokenized_val_dataset)):\n",
    "#     decoded_text = tokenizer.decode(\n",
    "#         tokenized_val_dataset[i][\"input_ids\"],\n",
    "#         skip_special_tokens=True,\n",
    "#         clean_up_tokenization_spaces=True,\n",
    "#     )\n",
    "#     b = list(decoded_texts)\n",
    "#     b.append(decoded_text)\n",
    "#     decoded_texts = tuple(b)\n",
    "# loaded(list(decoded_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ffe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall -y transformers\n",
    "# pip install transformers==4.51.3\n",
    "# pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d24d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-24 11:42:47,728: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-24 11:42:47,729: INFO: common: yaml file content: {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://drive.google.com/file/d/1taIeW6BZHqucmJbccEo92qfzZt5X_RTQ/view?usp=sharing', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'prepare_base_model': {'root_dir': 'artifacts/prepare_base_model', 'base_model_path': 'artifacts/prepare_base_model/base_model.h5', 'updated_base_model_path': 'artifacts/prepare_base_model/base_model_updated.h5', 'base_tokenizer_path': 'artifacts/prepare_base_tokenizer/base_model.h5', 'updated_base_tokenizer_path': 'artifacts/prepare_base_tokenizer/base_model_updated.h5'}, 'data_preprocessing': {'root_dir': 'artifacts/data_preprocessing', 'raw_data_file': 'artifacts/data_ingestion/bitext-travel-llm-chatbot-training-dataset.csv', 'train_data_path': 'artifacts/data_preprocessing/train.csv', 'test_data_path': 'artifacts/data_preprocessing/test.csv', 'val_data_path': 'artifacts/data_preprocessing/val.csv'}, 'text_processing': {'root_dir': 'artifacts/text_processing', 'train_tokenized_data_path': 'artifacts/text_processing/train_tokenized.csv', 'test_tokenized_data_path': 'artifacts/text_processing/test_tokenized.csv', 'val_tokenized_data_path': 'artifacts/text_processing/val_tokenized.csv'}, 'model_training': {'root_dir': 'artifacts/model_training', 'model_path': 'artifacts/model_training/model.h5', 'tokenizer_path': 'artifacts/model_training/tokenizer.h5', 'train_tokenized_data_path': 'artifacts/text_processing/train', 'test_tokenized_data_path': 'artifacts/text_processing/test', 'val_tokenized_data_path': 'artifacts/text_processing/val'}} ]\n",
      "[2025-05-24 11:42:47,729: INFO: common: datapreprocessing file content: {'root_dir': 'artifacts/data_preprocessing', 'raw_data_file': 'artifacts/data_ingestion/bitext-travel-llm-chatbot-training-dataset.csv', 'train_data_path': 'artifacts/data_preprocessing/train.csv', 'test_data_path': 'artifacts/data_preprocessing/test.csv', 'val_data_path': 'artifacts/data_preprocessing/val.csv'} ]\n",
      "[2025-05-24 11:42:47,734: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-24 11:42:47,735: INFO: common: yaml file content: {'CHECKPOINT': 'distilbert-base-uncased', 'NUM_LABELS': 33, 'MAX_LENGTH': 512, 'BATCH_SIZE': 8, 'EPOCHS': 3, 'MODEL_DIR': 'artifacts/model', 'MODEL_NAME': 'distilbert-base-uncased', 'TEST_SIZE': 0.2, 'TRAIN_SIZE': 0.8, 'SEED': 42, 'RANDOM_STATE': 42, 'LABEL_COL': 'intent', 'TEXT_COL': 'instruction', 'TRAINING_ARGUMENTS': {'eval_strategy': 'epoch', 'save_strategy': 'epoch', 'learning_rate': '2e-5', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'num_train_epochs': 10, 'weight_decay': 0.01, 'load_best_model_at_end': True, 'metric_for_best_model': 'f1'}} ]\n",
      "[2025-05-24 11:42:47,736: INFO: common: datapreprocessing file content: None ]\n",
      "[2025-05-24 11:42:47,737: INFO: common: created directory at: artifacts]\n",
      "Training arguments: {'eval_strategy': 'epoch', 'save_strategy': 'epoch', 'learning_rate': '2e-5', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'num_train_epochs': 10, 'weight_decay': 0.01, 'load_best_model_at_end': True, 'metric_for_best_model': 'f1'}\n",
      "[2025-05-24 11:42:47,738: INFO: common: created directory at: artifacts/model_training]\n",
      "Model training config:  ModelTrainingConfig(root_dir=WindowsPath('artifacts/model_training'), base_model_path=WindowsPath('artifacts/prepare_base_model/base_model.h5'), base_tokenizer_path=WindowsPath('artifacts/prepare_base_tokenizer/base_model.h5'), model_path=WindowsPath('artifacts/model_training/model.h5'), tokenizer_path=WindowsPath('artifacts/model_training/tokenizer.h5'), train_tokenized_data_path=WindowsPath('artifacts/text_processing/train'), test_tokenized_data_path=WindowsPath('artifacts/text_processing/test'), val_tokenized_data_path=WindowsPath('artifacts/text_processing/val'), params_model_name='distilbert-base-uncased', params_eval_strategy='epoch', params_save_strategy='epoch', params_learning_rate='2e-5', params_per_device_train_batch_size=8, params_per_device_eval_batch_size=8, params_num_train_epochs=10, params_weight_decay=0.01, params_load_best_model_at_end=True, params_metric_for_best_model='f1')\n",
      "ModelTrainingConfig(root_dir=WindowsPath('artifacts/model_training'), base_model_path=WindowsPath('artifacts/prepare_base_model/base_model.h5'), base_tokenizer_path=WindowsPath('artifacts/prepare_base_tokenizer/base_model.h5'), model_path=WindowsPath('artifacts/model_training/model.h5'), tokenizer_path=WindowsPath('artifacts/model_training/tokenizer.h5'), train_tokenized_data_path=WindowsPath('artifacts/text_processing/train'), test_tokenized_data_path=WindowsPath('artifacts/text_processing/test'), val_tokenized_data_path=WindowsPath('artifacts/text_processing/val'), params_model_name='distilbert-base-uncased', params_eval_strategy='epoch', params_save_strategy='epoch', params_learning_rate='2e-5', params_per_device_train_batch_size=8, params_per_device_eval_batch_size=8, params_num_train_epochs=10, params_weight_decay=0.01, params_load_best_model_at_end=True, params_metric_for_best_model='f1')\n",
      "*******************without peft*******************\n",
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "DistilBertForSequenceClassification                     --\n",
      "├─DistilBertModel: 1-1                                  --\n",
      "│    └─Embeddings: 2-1                                  --\n",
      "│    │    └─Embedding: 3-1                              (23,440,896)\n",
      "│    │    └─Embedding: 3-2                              (393,216)\n",
      "│    │    └─LayerNorm: 3-3                              (1,536)\n",
      "│    │    └─Dropout: 3-4                                --\n",
      "│    └─Transformer: 2-2                                 --\n",
      "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
      "├─Linear: 1-2                                           590,592\n",
      "├─Linear: 1-3                                           25,377\n",
      "├─Dropout: 1-4                                          --\n",
      "================================================================================\n",
      "Total params: 66,978,849\n",
      "Trainable params: 615,969\n",
      "Non-trainable params: 66,362,880\n",
      "================================================================================\n",
      "*******************Model*******************\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=33, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "*******************with peft*******************\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                                            Param #\n",
      "==========================================================================================\n",
      "PeftModelForSequenceClassification                                --\n",
      "├─LoraModel: 1-1                                                  --\n",
      "│    └─DistilBertForSequenceClassification: 2-1                   --\n",
      "│    │    └─DistilBertModel: 3-1                                  66,381,312\n",
      "│    │    └─ModulesToSaveWrapper: 3-2                             1,181,184\n",
      "│    │    └─ModulesToSaveWrapper: 3-3                             50,754\n",
      "│    │    └─Dropout: 3-4                                          --\n",
      "==========================================================================================\n",
      "Total params: 67,613,250\n",
      "Trainable params: 634,401\n",
      "Non-trainable params: 66,978,849\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\assi01\\AppData\\Local\\Temp\\ipykernel_4560\\4035347208.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='25330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    6/25330 00:32 < 57:40:40, 0.12 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run receptive-mole-652 at: https://dagshub.com/ashish.student2025/AirTravel_SentimentAnalysis.mlflow/#/experiments/0/runs/f1202a9e0070464da9688046af2110c4\n",
      "🧪 View experiment at: https://dagshub.com/ashish.student2025/AirTravel_SentimentAnalysis.mlflow/#/experiments/0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel training config: \u001b[39m\u001b[33m\"\u001b[39m, model_training_config)\n\u001b[32m      5\u001b[39m     model_training = ModelTraining(config=model_training_config)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mmodel_training\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mModelTraining.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m mlflow.set_experiment(\u001b[33m\"\u001b[39m\u001b[33mAir Travel Sentiment Analysis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run() \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     metrics = trainer.evaluate()\n\u001b[32m     87\u001b[39m     mlflow.log_metrics(metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:976\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    969\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    972\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    974\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    985\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    986\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:796\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    792\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    793\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    794\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:549\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    541\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    542\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    543\u001b[39m         hidden_state,\n\u001b[32m   (...)\u001b[39m\u001b[32m    546\u001b[39m         output_attentions,\n\u001b[32m    547\u001b[39m     )\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:475\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m \u001b[33;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m sa_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    484\u001b[39m     sa_output, sa_weights = sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:391\u001b[39m, in \u001b[36mDistilBertSdpaAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    389\u001b[39m q = shape(\u001b[38;5;28mself\u001b[39m.q_lin(query))  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[32m    390\u001b[39m k = shape(\u001b[38;5;28mself\u001b[39m.k_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m v = shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_lin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;66;03m# attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\u001b[39;00m\n\u001b[32m    395\u001b[39m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.require_contiguous_qkv \u001b[38;5;129;01mand\u001b[39;00m q.device.type == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:727\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m x = \u001b[38;5;28mself\u001b[39m._cast_input_dtype(x, lora_A.weight.dtype)\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_dora[active_adapter]:\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m     result = result + lora_B(lora_A(\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)) * scaling\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dropout, nn.Identity) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.get_model_training_config()\n",
    "    print(\"Model training config: \", model_training_config)\n",
    "    model_training = ModelTraining(config=model_training_config)\n",
    "    model_training.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81f8ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run mysterious-fowl-585 at: https://dagshub.com/ashish.student2025/AirTravel_SentimentAnalysis.mlflow/#/experiments/0/runs/88848d98e99746849e836ba934482684\n",
      "🧪 View experiment at: https://dagshub.com/ashish.student2025/AirTravel_SentimentAnalysis.mlflow/#/experiments/0\n",
      "Ended existing MLflow run\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mlflow.end_run()\n",
    "    print(\"Ended existing MLflow run\")\n",
    "except:\n",
    "    print(\"No active MLflow run to end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Transformers location:\", transformers.__file__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
