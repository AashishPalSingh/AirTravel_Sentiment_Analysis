{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65805bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a751303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\assi01\\\\Desktop\\\\projects\\\\AirTravel_Sentiment_Analysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecfa4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504e3f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\assi01\\\\Desktop\\\\projects\\\\AirTravel_Sentiment_Analysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d841e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path\n",
    "    base_tokenizer_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    train_tokenized_data_path: Path\n",
    "    test_tokenized_data_path: Path\n",
    "    val_tokenized_data_path: Path\n",
    "    params_model_name: str\n",
    "    params_evaluation_strategy: str\n",
    "    params_save_strategy: str\n",
    "    params_learning_rate: float\n",
    "    params_per_device_train_batch_size: int\n",
    "    params_per_device_eval_batch_size: int\n",
    "    params_num_train_epochs: int\n",
    "    params_weight_decay: float\n",
    "    params_load_best_model_at_end: bool\n",
    "    params_metric_for_best_model: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a849d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airTravelSentimentAnalysis.constants import *\n",
    "from airTravelSentimentAnalysis.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "422ca28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        config[\"base_model_path\"] = self.config.prepare_base_model.base_model_path\n",
    "        config[\"base_tokenizer_path\"] = self.config.prepare_base_model.base_tokenizer_path\n",
    "        params_training = self.params.TRAINING_ARGUMENTS\n",
    "        print(f\"Training arguments: {params_training}\")\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            base_model_path=Path(config.base_model_path),\n",
    "            base_tokenizer_path=Path(config.base_tokenizer_path),\n",
    "            model_path=Path(config.model_path),\n",
    "            tokenizer_path=Path(config.tokenizer_path),\n",
    "            train_tokenized_data_path=Path(config.train_tokenized_data_path),\n",
    "            test_tokenized_data_path=Path(config.test_tokenized_data_path),\n",
    "            val_tokenized_data_path=Path(config.val_tokenized_data_path),\n",
    "            params_model_name=self.params.MODEL_NAME,\n",
    "            params_evaluation_strategy=params_training.evaluation_strategy,\n",
    "            params_save_strategy=params_training.save_strategy,\n",
    "            params_learning_rate=params_training.learning_rate,\n",
    "            params_per_device_train_batch_size=params_training.per_device_train_batch_size,\n",
    "            params_per_device_eval_batch_size=params_training.per_device_eval_batch_size,\n",
    "            params_num_train_epochs=params_training.num_train_epochs,\n",
    "            params_weight_decay=params_training.weight_decay,\n",
    "            params_load_best_model_at_end=params_training.load_best_model_at_end,\n",
    "            params_metric_for_best_model=params_training.metric_for_best_model,\n",
    "        )\n",
    "\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcbccaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits,axis=-1)  # Predicted class is the index of max logit\n",
    "        precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "        recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "        f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    def train(self):\n",
    "        print(self.config)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.model_path,eval_strategy =\"epoch\",\n",
    "            save_strategy=self.config.params_save_strategy,\n",
    "            # learning_rate=self.config.params_learning_rate,\n",
    "            per_device_train_batch_size=self.config.params_per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.params_per_device_eval_batch_size,\n",
    "            num_train_epochs=self.config.params_num_train_epochs,\n",
    "            weight_decay=self.config.params_weight_decay,\n",
    "            load_best_model_at_end=self.config.params_load_best_model_at_end,\n",
    "            metric_for_best_model=self.config.params_metric_for_best_model\n",
    "        )   \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.base_model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.base_tokenizer_path)\n",
    "        tokenized_train_dataset = load_from_disk(self.config.train_tokenized_data_path)\n",
    "        tokenized_train_dataset = tokenized_train_dataset.rename_column(\"intent\", \"labels\")\n",
    "        tokenized_test_dataset = load_from_disk(self.config.test_tokenized_data_path)   \n",
    "        tokenized_val_dataset = load_from_disk(self.config.val_tokenized_data_path)\n",
    "        tokenized_val_dataset = tokenized_val_dataset.rename_column(\"intent\", \"labels\")\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train_dataset,\n",
    "            eval_dataset=tokenized_val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        trainer.save_model(self.config.model_path)\n",
    "        tokenizer.save_pretrained(self.config.tokenizer_path)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d24d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-23 01:22:17,414: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-23 01:22:17,415: INFO: common: yaml file content: {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://drive.google.com/file/d/1taIeW6BZHqucmJbccEo92qfzZt5X_RTQ/view?usp=sharing', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'prepare_base_model': {'root_dir': 'artifacts/prepare_base_model', 'base_model_path': 'artifacts/prepare_base_model/base_model.h5', 'updated_base_model_path': 'artifacts/prepare_base_model/base_model_updated.h5', 'base_tokenizer_path': 'artifacts/prepare_base_tokenizer/base_model.h5', 'updated_base_tokenizer_path': 'artifacts/prepare_base_tokenizer/base_model_updated.h5'}, 'data_preprocessing': {'root_dir': 'artifacts/data_preprocessing', 'raw_data_file': 'artifacts/data_ingestion/bitext-travel-llm-chatbot-training-dataset.csv', 'train_data_path': 'artifacts/data_preprocessing/train.csv', 'test_data_path': 'artifacts/data_preprocessing/test.csv', 'val_data_path': 'artifacts/data_preprocessing/val.csv'}, 'text_processing': {'root_dir': 'artifacts/text_processing', 'train_tokenized_data_path': 'artifacts/text_processing/train_tokenized.csv', 'test_tokenized_data_path': 'artifacts/text_processing/test_tokenized.csv', 'val_tokenized_data_path': 'artifacts/text_processing/val_tokenized.csv'}, 'model_training': {'root_dir': 'artifacts/model_training', 'model_path': 'artifacts/model_training/model.h5', 'tokenizer_path': 'artifacts/model_training/tokenizer.h5', 'train_tokenized_data_path': 'artifacts/text_processing/train', 'test_tokenized_data_path': 'artifacts/text_processing/test', 'val_tokenized_data_path': 'artifacts/text_processing/val'}} ]\n",
      "[2025-05-23 01:22:17,415: INFO: common: datapreprocessing file content: {'root_dir': 'artifacts/data_preprocessing', 'raw_data_file': 'artifacts/data_ingestion/bitext-travel-llm-chatbot-training-dataset.csv', 'train_data_path': 'artifacts/data_preprocessing/train.csv', 'test_data_path': 'artifacts/data_preprocessing/test.csv', 'val_data_path': 'artifacts/data_preprocessing/val.csv'} ]\n",
      "[2025-05-23 01:22:17,418: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-23 01:22:17,419: INFO: common: yaml file content: {'CHECKPOINT': 'distilbert-base-uncased', 'NUM_LABELS': 33, 'MAX_LENGTH': 512, 'BATCH_SIZE': 16, 'EPOCHS': 3, 'MODEL_DIR': 'artifacts/model', 'MODEL_NAME': 'distilbert-base-uncased', 'TEST_SIZE': 0.2, 'TRAIN_SIZE': 0.8, 'SEED': 42, 'RANDOM_STATE': 42, 'LABEL_COL': 'intent', 'TEXT_COL': 'instruction', 'TRAINING_ARGUMENTS': {'evaluation_strategy': 'epoch', 'save_strategy': 'epoch', 'learning_rate': '2e-5', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.01, 'load_best_model_at_end': True, 'metric_for_best_model': 'f1'}} ]\n",
      "[2025-05-23 01:22:17,419: INFO: common: datapreprocessing file content: None ]\n",
      "[2025-05-23 01:22:17,420: INFO: common: created directory at: artifacts]\n",
      "Training arguments: {'evaluation_strategy': 'epoch', 'save_strategy': 'epoch', 'learning_rate': '2e-5', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.01, 'load_best_model_at_end': True, 'metric_for_best_model': 'f1'}\n",
      "[2025-05-23 01:22:17,422: INFO: common: created directory at: artifacts/model_training]\n",
      "ModelTrainingConfig(root_dir=WindowsPath('artifacts/model_training'), base_model_path=WindowsPath('artifacts/prepare_base_model/base_model.h5'), base_tokenizer_path=WindowsPath('artifacts/prepare_base_tokenizer/base_model.h5'), model_path=WindowsPath('artifacts/model_training/model.h5'), tokenizer_path=WindowsPath('artifacts/model_training/tokenizer.h5'), train_tokenized_data_path=WindowsPath('artifacts/text_processing/train'), test_tokenized_data_path=WindowsPath('artifacts/text_processing/test'), val_tokenized_data_path=WindowsPath('artifacts/text_processing/val'), params_model_name='distilbert-base-uncased', params_evaluation_strategy='epoch', params_save_strategy='epoch', params_learning_rate='2e-5', params_per_device_train_batch_size=8, params_per_device_eval_batch_size=8, params_num_train_epochs=2, params_weight_decay=0.01, params_load_best_model_at_end=True, params_metric_for_best_model='f1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\assi01\\AppData\\Local\\Temp\\ipykernel_15824\\3176620925.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='5066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 225/5066 03:11 < 1:09:13, 1.17 it/s, Epoch 0.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     model_training_config = config.get_model_training_config()\n\u001b[32m      4\u001b[39m     model_training = ModelTraining(config=model_training_config)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mmodel_training\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mModelTraining.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     41\u001b[39m tokenized_val_dataset = tokenized_val_dataset.rename_column(\u001b[33m\"\u001b[39m\u001b[33mintent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m trainer = Trainer(\n\u001b[32m     43\u001b[39m     model=model,\n\u001b[32m     44\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     compute_metrics=\u001b[38;5;28mself\u001b[39m.compute_metrics\n\u001b[32m     49\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m trainer.save_model(\u001b[38;5;28mself\u001b[39m.config.model_path)\n\u001b[32m     52\u001b[39m tokenizer.save_pretrained(\u001b[38;5;28mself\u001b[39m.config.tokenizer_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\transformers\\trainer.py:2588\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2583\u001b[39m     _grad_norm = nn.utils.clip_grad_norm_(\n\u001b[32m   2584\u001b[39m         amp.master_params(\u001b[38;5;28mself\u001b[39m.optimizer),\n\u001b[32m   2585\u001b[39m         args.max_grad_norm,\n\u001b[32m   2586\u001b[39m     )\n\u001b[32m   2587\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2588\u001b[39m     _grad_norm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2590\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2594\u001b[39m     is_accelerate_available()\n\u001b[32m   2595\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED\n\u001b[32m   2596\u001b[39m ):\n\u001b[32m   2597\u001b[39m     grad_norm = model.get_global_grad_norm()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:2629\u001b[39m, in \u001b[36mAccelerator.clip_grad_norm_\u001b[39m\u001b[34m(self, parameters, max_norm, norm_type)\u001b[39m\n\u001b[32m   2627\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m model.clip_grad_norm_(max_norm, norm_type)\n\u001b[32m   2628\u001b[39m \u001b[38;5;28mself\u001b[39m.unscale_gradients()\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:219\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    217\u001b[39m     parameters = \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[32m    218\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\assi01\\Desktop\\projects\\AirTravel_Sentiment_Analysis\\venv\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:91\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_tensors], _) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors.items():\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_tensors, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m     89\u001b[39m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[32m     90\u001b[39m     ):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         norms.extend(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     94\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.get_model_training_config()\n",
    "    model_training = ModelTraining(config=model_training_config)\n",
    "    model_training.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a074b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
